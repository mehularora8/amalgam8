{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pydub librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openunmix soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!ffmpeg -version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bpm(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the BPM of an audio file using librosa.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated BPM of the audio.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "\n",
    "    # Estimate the tempo\n",
    "    tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "\n",
    "    return tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_bpm(audio, original_bpm, target_bpm):\n",
    "    \"\"\"\n",
    "    Changes the BPM of an audio file by adjusting the speed.\n",
    "    This will also change the pitch of the audio.\n",
    "\n",
    "    Args:\n",
    "    audio (AudioSegment): The audio segment to adjust.\n",
    "    original_bpm (float): The original BPM of the audio.\n",
    "    target_bpm (float): The target BPM to achieve.\n",
    "\n",
    "    Returns:\n",
    "    AudioSegment: The modified audio segment with new BPM.\n",
    "    \"\"\"\n",
    "    ratio = target_bpm / original_bpm\n",
    "\n",
    "    new_audio = audio.speedup(playback_speed=ratio)\n",
    "\n",
    "    return new_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from openunmix import predict\n",
    "import numpy as np\n",
    "\n",
    "def separate_vocals(input_file, output_dir):\n",
    "    # Load the audio file\n",
    "    audio, rate = librosa.load(input_file, sr=44100, mono=False)\n",
    "    \n",
    "    # If the audio is mono, convert it to stereo\n",
    "    if audio.ndim == 1:\n",
    "        audio = np.repeat(audio.reshape(1, -1), 2, axis=0)\n",
    "    \n",
    "    # Separate the audio\n",
    "    estimates = predict.separate(torch.tensor(audio[None]), rate=rate)\n",
    "    \n",
    "    # Save the separated tracks\n",
    "    for source, estimate in estimates.items():\n",
    "        sf.write(f\"{output_dir}/{source}.mp3\", estimate[0].T, rate)\n",
    "    \n",
    "    print(f\"Separated tracks saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(trackList: list):\n",
    "    \"\"\"\n",
    "    Overlays multiple audio tracks together.\n",
    "\n",
    "    Args:\n",
    "    trackList (list): List of audio tracks to overlay.\n",
    "\n",
    "    Returns:\n",
    "    mixed_audio (AudioSegment): The mixed audio segment.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(trackList[0], \"mp3\")\n",
    "\n",
    "    for track in trackList[1:]:\n",
    "        audio2 = AudioSegment.from_file(track, \"mp3\")\n",
    "        audio = audio.overlay(audio2)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def mix(track1, track2, track1_start, track1_duration, track2_mix_entry, track2_start, track2_duration, overlap_time, offset):\n",
    "    \"\"\"\n",
    "    Mixes two audio tracks together with a specified overlap time.\n",
    "\n",
    "    Args:\n",
    "    track1 (str): Path to the first audio track.\n",
    "    track2 (str): Path to the second audio track.\n",
    "    track1_duration (float): Duration of snippet from first track (in seconds). Includes duration to be used in the overlap.\n",
    "    track2_duration (float): Duration of snippet from first track (in seconds).\n",
    "    overlap_time (float): Duration of the overlap in seconds.\n",
    "    track1_start (float): Start time of the first track in seconds.\n",
    "    track2_start (float): Start time of the second track\n",
    "\n",
    "    Intermediary Values:\n",
    "    mix: Fadeout of Song 1 + Overlay + Fadein of Song 2\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    final_mix (AudioSegment): The mixed audio segment.\n",
    "    \"\"\"\n",
    "    audio1 = AudioSegment.from_file(track1, \"wav\")\n",
    "    audio2 = AudioSegment.from_file(track2, \"wav\")\n",
    "\n",
    "    rampup = audio1[track1_start: track1_start + track1_duration - overlap_time]\n",
    "\n",
    "    if not os.path.exists(\"t1/vocals.mp3\"):\n",
    "        separate_vocals(track1, \"t1\")\n",
    "    if not os.path.exists(\"t2/vocals.mp3\"):\n",
    "        separate_vocals(track2, \"t2\")\n",
    "\n",
    "    t1_vocals = AudioSegment.from_file(\"t1/vocals.mp3\", \"mp3\")\n",
    "    t2_music = overlay(['t2/bass.mp3', 't2/drums.mp3', 't2/other.mp3'])\n",
    "    \n",
    "    rampdown = audio2[track2_start: track2_start + track2_duration]\n",
    "\n",
    "    mix_vocals = t1_vocals[track1_start + track1_duration - overlap_time: track1_start + track1_duration]\n",
    "    mix_music = AudioSegment.silent(duration=offset) + t2_music[track2_mix_entry: track2_mix_entry + overlap_time - offset]\n",
    "    mix_audio = mix_music.overlay(mix_vocals)\n",
    "    \n",
    "    combined_mix = rampup + mix_audio + rampdown\n",
    "\n",
    "    return combined_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change these values to match the input audio files. Might need some finetuning to get right.\n",
    "track1_file = \"baby.mp4\"\n",
    "track2_file = \"dk.mp4\"\n",
    "track1_start = 2200\n",
    "track1_duration = 20100\n",
    "track2_mix_entry = 6100\n",
    "track2_start = 20800\n",
    "track2_duration = 8500\n",
    "overlap_time = 15450\n",
    "offset = 800\n",
    "\n",
    "final_mix = mix(track1_file, track2_file, track1_start, track1_duration, track2_mix_entry, track2_start, track2_duration, overlap_time, offset)\n",
    "final_mix.export(\"mix.mp3\", format=\"mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
